{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from gvit.quadtree import QuadTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('/Users/zhangenzhi/work/gvit-1/dataset/exp/paip.png')\n",
    "grey_img = img[0:512,0:512, 0]\n",
    "edges = cv.Canny(grey_img, 100,200)\n",
    "\n",
    "plt.subplot(121),plt.imshow(grey_img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.savefig(fname=\"paip_canny.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(edges, cmap = 'gray')\n",
    "\n",
    "qdt = QuadTree(domain=edges)\n",
    "qdt.draw(ax=ax)\n",
    "plt.savefig(fname=\"paip_pachify.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8x8 => 868 patches\n",
    "# 64*64 => 4096 patches  \n",
    "\n",
    "patch_info = {}\n",
    "print(qdt.count_patches(patch_info))\n",
    "print(patch_info)\n",
    "print(sum(patch_info.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save patch sequence\n",
    "def compress_mix_patches(qdt:QuadTree, img: np.array, to_size:tuple = (8,8,3)):\n",
    "    h2,w2,c2 = to_size\n",
    "    seq_patches = qdt.serialize(img=img[0:512,0:512])\n",
    "    for i in range(len(seq_patches)):\n",
    "        h1, w1, c1 = seq_patches[i].shape\n",
    "        assert h1==w1, \"Need squared input.\"\n",
    "        # print(seq_patches[i].shape, seq_patches[i])\n",
    "        step =int(h1/to_size[0])\n",
    "        seq_patches[i] = seq_patches[i][::step,::step]\n",
    "        assert seq_patches[i].shape == (h2,w2,c2), \"Wrong shape {} get, need {}\".format(seq_patches[i].shape, (h2,w2,c2))\n",
    "    return seq_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_patches = compress_mix_patches(qdt=qdt,img=img,to_size=(8,8,3))\n",
    "print(len(seq_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet patchify\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "datapath = './dataset'\n",
    "def imagenet(datapath=datapath):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor()  \n",
    "        ])\n",
    "\n",
    "    imagenet_data = torchvision.datasets.ImageNet('./dataset', transform= train_transform)\n",
    "    imagenet_val = torchvision.datasets.ImageNet('./dataset', split=\"val\", transform= train_transform)\n",
    "    print(\"train samples:{}, val_samples:{}\".format(len(imagenet_data), len(imagenet_val)))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(imagenet_data,\n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=64)\n",
    "    val_loader = torch.utils.data.DataLoader(imagenet_val,\n",
    "                                        batch_size=4,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=64)\n",
    "    \n",
    "    train_dataset = iter(train_loader)\n",
    "    val_dataset = iter(val_loader)\n",
    "    return train_dataset, val_loader\n",
    "\n",
    "train_ds,val_ds = imagenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset patchify\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "datapath = './dataset/exp/'\n",
    "def custom_dataset(datapath=datapath):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor()  \n",
    "        ])\n",
    "\n",
    "    image_train = torchvision.datasets.ImageFolder(datapath, transform= train_transform)\n",
    "    image_val = torchvision.datasets.ImageFolder(datapath, transform= train_transform)\n",
    "    print(\"train samples:{}, val_samples:{}\".format(len(image_train), len(image_val)))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(image_train,\n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=64)\n",
    "    val_loader = torch.utils.data.DataLoader(image_val,\n",
    "                                        batch_size=4,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=64)\n",
    "    \n",
    "    train_dataset = iter(train_loader)\n",
    "    val_dataset = iter(val_loader)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_ds,val_ds = custom_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
